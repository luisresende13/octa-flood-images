{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eccf1eb-e642-45dc-b45f-bb0097d1e804",
   "metadata": {},
   "source": [
    "#### Reload images and videos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc1797b-f399-45de-899c-24ebfb834d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'blob_name', 'blob_size', 'bucket_name', 'file_name', 'code',\n",
      "       'n_folders', 'timestamp', 'folder_structure', 'folder', 'tags', 'url',\n",
      "       'api_url', 'bucket', 'seen'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=10, step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_images = pd.read_csv('data/datasets/images.csv')\n",
    "df_videos = pd.read_csv('data/datasets/videos.csv')\n",
    "\n",
    "# df_images['tags'] = df_images['tags'].apply(json.loads)\n",
    "df_videos['tags'] = df_videos['tags'].apply(json.loads)\n",
    "\n",
    "print(df_videos.columns)\n",
    "print()\n",
    "display(df_videos.index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027136bd-1148-46e5-8076-15cba484525c",
   "metadata": {},
   "source": [
    "#### Create tag field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589f97e0-2039-465d-b7ad-e0fc1e7564f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "normal        60241\n",
       "poça           1348\n",
       "lâmina          214\n",
       "bolsão           96\n",
       "transbordo       62\n",
       "alagamento       56\n",
       "Name: Videos, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag\n",
       "normal        100031\n",
       "poça           53218\n",
       "lâmina          7979\n",
       "bolsão          3612\n",
       "transbordo      2237\n",
       "alagamento      1417\n",
       "Name: Images, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modules.octa_video_util import _assign_tag\n",
    "\n",
    "# Create unique tag column based on class priority list\n",
    "default_tag = 'normal'\n",
    "tags_priority_list = ['alagamento', 'bolsão', 'lâmina', 'poça', 'transbordo']\n",
    "\n",
    "df_videos['tag'] = df_videos['tags'].apply(lambda tags_list: _assign_tag(tags_list, tags_priority_list, default_tag))\n",
    "df_images['tag'] = df_images['tags'].apply(lambda tags_list: _assign_tag(tags_list, tags_priority_list, default_tag))\n",
    "\n",
    "display(df_videos.tag.value_counts().rename('Videos'))\n",
    "print()\n",
    "display(df_images.tag.value_counts().rename('Images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116e83a-5d9b-4b84-8892-5e776eaf25b1",
   "metadata": {},
   "source": [
    "#### Binarize 'tag' variable for images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954d1006-7d56-4abf-b2bd-b008f59bb7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flood\n",
       "0    155486\n",
       "1     13008\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=10, step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_classes = ['lâmina', 'bolsão', 'alagamento']\n",
    "\n",
    "# Binarize categorical variable from list of target classes\n",
    "df_images['flood'] = df_images['tag'].isin(target_classes).astype(int)\n",
    "\n",
    "display(df_images['flood'].value_counts())\n",
    "print()\n",
    "display(df_images.index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a59c7-0be8-4fd6-bbca-c79c5c917c6b",
   "metadata": {},
   "source": [
    "#### Filter videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2722dd04-8381-4899-81e9-9c2f443bd86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "normal        2669\n",
       "poça          1348\n",
       "lâmina         213\n",
       "bolsão          94\n",
       "alagamento      36\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modules.octa_video_util import filter_by_query\n",
    "\n",
    "query_params = {\n",
    "    'seen': True,\n",
    "    'tag': ['normal', 'poça', 'lâmina', 'bolsão', 'alagamento']\n",
    "}\n",
    "\n",
    "# Filter dataset of images by query\n",
    "df_videos_filtered = filter_by_query(df_videos, query_params).copy()\n",
    "\n",
    "display(df_videos_filtered['tag'].value_counts())\n",
    "print()\n",
    "display(df_videos_filtered.index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a04a2b-8498-4902-9820-f9221eab2a98",
   "metadata": {},
   "source": [
    "#### Reload sample images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d041d226-ae47-470b-8d0f-21b09c9c370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_video', 'code', 'folder', 'file_name', 'file_path', 'frame_index',\n",
      "       'timestamp', 'initial_timestamp', 'seen', 'tags', 'tag', 'flood',\n",
      "       'set'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([152562, 28728, 102704, 5237, 112104, 19950, 8079, 37094, 127490, 53874], dtype='int64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_directory = 'data/splits/sgkf-8-1-1'\n",
    "\n",
    "df_sample = pd.read_csv(f'{target_directory}/images.csv', index_col=0)\n",
    "\n",
    "print(df_sample.columns)\n",
    "display(df_sample.index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4715832-9428-46f7-a86c-3f4f6c725577",
   "metadata": {},
   "source": [
    "#### Unique cameras in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52459e53-ed82-4ddc-992c-323f1316113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train codes: 114\n",
      "Test codes: 15\n",
      "Val codes: 9\n",
      "Out-train codes: 58\n"
     ]
    }
   ],
   "source": [
    "train = df_sample[df_sample['set']=='train']\n",
    "test = df_sample[df_sample['set']=='test']\n",
    "val = df_sample[df_sample['set']=='val']\n",
    "\n",
    "train_codes = train['code'].unique()\n",
    "test_codes = test['code'].unique()\n",
    "val_codes = val['code'].unique()\n",
    "out_train_codes = set(df_videos_filtered['code'].unique()).difference(train_codes)\n",
    "\n",
    "print('Train codes:', len(train_codes))\n",
    "print('Test codes:', len(test_codes))\n",
    "print('Val codes:', len(val_codes))\n",
    "print('Out-train codes:', len(out_train_codes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a1aa5-bb63-4286-930a-fb5b5b49e378",
   "metadata": {},
   "source": [
    "#### Test videos from cameras outside the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c98ab9-38ee-4b9a-b8c3-60c7660e3b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "normal        800\n",
       "poça          336\n",
       "lâmina         34\n",
       "bolsão         20\n",
       "alagamento      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([56, 57, 58, 59, 60, 61, 62, 63, 64, 65], dtype='int64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of sample for testing: (1197, 16)\n"
     ]
    }
   ],
   "source": [
    "df_videos_test = df_videos_filtered[~df_videos_filtered['code'].isin(train_codes)]\n",
    "\n",
    "display(df_videos_test['tag'].value_counts())\n",
    "print()\n",
    "display(df_videos_test.index[:10])\n",
    "print()\n",
    "print('Shape of sample for testing:', df_videos_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2b806-9494-4ade-8d81-8b8fc08ade4a",
   "metadata": {},
   "source": [
    "#### Undersample videos based on flood event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ddf3e5-3da5-4aec-bfe7-e7937904bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos selected: (81, 16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag\n",
       "lâmina        34\n",
       "bolsão        20\n",
       "poça          12\n",
       "normal         8\n",
       "alagamento     7\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>code</th>\n",
       "      <th>278.0</th>\n",
       "      <th>1460.0</th>\n",
       "      <th>3290.0</th>\n",
       "      <th>482.0</th>\n",
       "      <th>384.0</th>\n",
       "      <th>1547.0</th>\n",
       "      <th>2166.0</th>\n",
       "      <th>299.0</th>\n",
       "      <th>1431.0</th>\n",
       "      <th>1393.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1601.0</th>\n",
       "      <th>2156.0</th>\n",
       "      <th>226.0</th>\n",
       "      <th>1487.0</th>\n",
       "      <th>1507.0</th>\n",
       "      <th>1525.0</th>\n",
       "      <th>230.0</th>\n",
       "      <th>313.0</th>\n",
       "      <th>1881.0</th>\n",
       "      <th>112.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "code   278.0   1460.0  3290.0  482.0   384.0   1547.0  2166.0  299.0   1431.0  \\\n",
       "count      26      23       7       3       3       2       2       2       1   \n",
       "\n",
       "code   1393.0  ...  1601.0  2156.0  226.0   1487.0  1507.0  1525.0  230.0   \\\n",
       "count       1  ...       1       1       1       1       1       1       1   \n",
       "\n",
       "code   313.0   1881.0  112.0   \n",
       "count       1       1       1  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([486, 571, 572, 573, 1029, 1030, 18477, 18478, 18569, 18671], dtype='int64')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_majority_videos = 20\n",
    "random_state = 0\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "target_classes = ['lâmina', 'bolsão', 'alagamento']\n",
    "test_minority = df_videos_test[df_videos_test['tag'].isin(target_classes)]\n",
    "test_majority = df_videos_test.drop(test_minority.index)\n",
    "\n",
    "x = test_majority.copy()\n",
    "y = test_majority['tag'].copy()\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=random_state, replacement=False)\n",
    "\n",
    "test_majority_filtered, tags_test_majority_filtered = rus.fit_resample(x, y)\n",
    "test_majority_filtered = test_majority_filtered.sample(n_majority_videos, replace=False, random_state=random_state)\n",
    "\n",
    "df_videos_test_filtered = pd.concat([test_minority, test_majority_filtered])\n",
    "\n",
    "print('Videos selected:', df_videos_test_filtered.shape)\n",
    "print()\n",
    "display(df_videos_test_filtered['tag'].value_counts())\n",
    "print()\n",
    "display(df_videos_test_filtered['code'].value_counts().to_frame().T)\n",
    "print()\n",
    "display(df_videos_test_filtered.index[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b423dc6-e17b-48b0-8cf6-af125cd518f6",
   "metadata": {},
   "source": [
    "#### Extract corresponding rows from images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef91a76-6fd1-4134-b620-87fb3743c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images selected: (3089, 12)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag\n",
       "lâmina        1214\n",
       "bolsão         900\n",
       "poça           461\n",
       "alagamento     291\n",
       "normal         223\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>code</th>\n",
       "      <th>1460.0</th>\n",
       "      <th>278.0</th>\n",
       "      <th>3290.0</th>\n",
       "      <th>384.0</th>\n",
       "      <th>482.0</th>\n",
       "      <th>2166.0</th>\n",
       "      <th>1547.0</th>\n",
       "      <th>299.0</th>\n",
       "      <th>1601.0</th>\n",
       "      <th>2156.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1881.0</th>\n",
       "      <th>226.0</th>\n",
       "      <th>1393.0</th>\n",
       "      <th>112.0</th>\n",
       "      <th>1403.0</th>\n",
       "      <th>1431.0</th>\n",
       "      <th>230.0</th>\n",
       "      <th>313.0</th>\n",
       "      <th>1525.0</th>\n",
       "      <th>1487.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1035</td>\n",
       "      <td>884</td>\n",
       "      <td>291</td>\n",
       "      <td>135</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>61</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "code   1460.0  278.0   3290.0  384.0   482.0   2166.0  1547.0  299.0   1601.0  \\\n",
       "count    1035     884     291     135     120      90      77      61      45   \n",
       "\n",
       "code   2156.0  ...  1881.0  226.0   1393.0  112.0   1403.0  1431.0  230.0   \\\n",
       "count      45  ...      45      45      30      30      30      28      19   \n",
       "\n",
       "code   313.0   1525.0  1487.0  \n",
       "count      15      11       8  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Videos found: 81\n"
     ]
    }
   ],
   "source": [
    "df_images_sample = df_images[df_images['id_video'].isin(df_videos_test_filtered['_id'])]\n",
    "\n",
    "print('Images selected:', df_images_sample.shape)\n",
    "print()\n",
    "display(df_images_sample['tag'].value_counts())\n",
    "print()\n",
    "display(df_images_sample['code'].value_counts().to_frame().T)\n",
    "print()\n",
    "print('Videos found:', len(df_images_sample['id_video'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9157967-75e6-43b9-b28b-60d2657409a0",
   "metadata": {},
   "source": [
    "#### Check existence of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a10b365-5d95-494d-adf6-ca3992d6a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.0 %'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_nested_files(folder_path):\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path.replace('\\\\', '/'))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# Example usage\n",
    "# base_path = 'data/images'\n",
    "# images_paths_found = get_nested_files(base_path)\n",
    "\n",
    "\n",
    "base_path = 'data/images'\n",
    "\n",
    "images_paths_found = get_nested_files(base_path)\n",
    "images_paths_sample = df_images_sample['file_path'].apply(lambda file_path: f'{base_path}/{file_path}'.replace('\\\\', '/'))\n",
    "files_exist_prct = images_paths_sample.isin(images_paths_found).mean()\n",
    "\n",
    "str(round(files_exist_prct * 100, 2)) + ' %'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313acba-ab11-4d68-9772-ce78eef3720a",
   "metadata": {},
   "source": [
    "#### Load model with Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb3573f-4e8d-46d8-8f47-90934f0bd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the folder you want to zip\n",
    "folder_path = f'YoloV8/train-results/sgkf-8-1-1-epochs23'\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(f'{folder_path}/weights/best.pt')  # load a partially trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa45a4-3f99-498d-b4ea-72db58e76bea",
   "metadata": {},
   "source": [
    "#### Run predictions with yolo in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcda6d5d-6673-4efb-816b-4b44e4a66ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088/3089 | 30.55 min / 30.56 min | time-left: 0.01 min\n",
      "\n",
      "0: 640x640 0 0.99, 1 0.01, 612.9ms\n",
      "Speed: 30.0ms preprocess, 612.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output as co\n",
    "import time\n",
    "\n",
    "base_path = 'data/images'\n",
    "batch = 8\n",
    "\n",
    "eval_imgs_df = df_images_sample.sort_values(['id_video', 'frame_index'])# .head(45)\n",
    "img_path_list = f'{base_path}/' + eval_imgs_df['file_path']\n",
    "n_imgs = len(img_path_list)\n",
    "\n",
    "preds = []\n",
    "avg_time = 0.0\n",
    "s_time = time.time()\n",
    "for i in range(0, n_imgs, batch):\n",
    "    e_time = time.time() - s_time\n",
    "    e_time_round = round(e_time / 60, 2)\n",
    "    avg_time = e_time / max(1, i)\n",
    "    expected_finish_time = round((n_imgs - i) * avg_time / 60, 2)\n",
    "    expected_total_time = round(n_imgs * avg_time  / 60, 2)\n",
    "\n",
    "    co(True)\n",
    "    print(f'{i}/{n_imgs} | {e_time_round} min / {expected_total_time} min | time-left: {expected_finish_time} min')    \n",
    "\n",
    "    img_path_list_batch = img_path_list[i: i + batch].tolist()\n",
    "    pred = model.predict(img_path_list_batch, imgsz=640)\n",
    "    pred = [[pred_i.probs.top1, pred_i.probs.data[1].item()] for pred_i in pred]\n",
    "    preds.extend(pred)\n",
    "\n",
    "eval_imgs_df[['pred', 'prob']] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361110ab-131e-472a-b09e-42bc01331c84",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9a4b91-e3ad-446b-a97f-8adbfb61d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_directory = 'data/splits/sgkf-8-1-1'\n",
    "eval_imgs_df.to_csv(f'{target_directory}/images-out-cameras.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6146506-fff7-48b7-8541-dd231af79433",
   "metadata": {},
   "source": [
    "#### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53897ab-c5ad-4fd1-8c2b-7bc6581b2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Confusion_matrix:\n",
      "[[ 418  266]\n",
      " [ 694 1711]]\n",
      "\n",
      " * Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.61      0.47       684\n",
      "           1       0.87      0.71      0.78      2405\n",
      "\n",
      "    accuracy                           0.69      3089\n",
      "   macro avg       0.62      0.66      0.62      3089\n",
      "weighted avg       0.76      0.69      0.71      3089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "labels = eval_imgs_df['flood'].tolist()\n",
    "pred_labels = [pred[0] for pred in preds]\n",
    "\n",
    "print('\\n * Confusion_matrix:')\n",
    "print(confusion_matrix(labels, pred_labels))\n",
    "print('\\n * Classification_report:')\n",
    "print(classification_report(labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9575c-b607-4fa9-8d73-6c9155a6cb34",
   "metadata": {},
   "source": [
    "#### Function to write video results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3483c511-ae42-41ba-86ac-d1d029a0457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def create_annotated_video(image_paths, true_labels, predicted_labels, predicted_probs, output_video_path, report=True):\n",
    "    if len(image_paths) != len(true_labels) or len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Number of paths, true labels, and predicted labels must be the same.\")\n",
    "\n",
    "    # Create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, 3.0, (854, 480)) # (640, 480)\n",
    "\n",
    "    n_imgs = len(image_paths)\n",
    "    for i, (image_path, true_label, predicted_label, predicted_prob) in enumerate(zip(image_paths, true_labels, predicted_labels, predicted_probs)):\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Set color based on prediction correctness\n",
    "        if true_label == predicted_label:\n",
    "            color = (0, 255, 0)  # Green for correct predictions\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Red for incorrect predictions\n",
    "\n",
    "        # Annotate the image with true and predicted labels in the bottom-left corner\n",
    "        true_label_text = f'True: {true_label}'\n",
    "        predicted_label_text = f'Predicted: {int(predicted_label)}'\n",
    "        probability_text = f'Probability: {round(predicted_prob * 100, 1)} %'\n",
    "        cv2.putText(image, true_label_text, (image.shape[1] - 130, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, predicted_label_text, (image.shape[1] - 210, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, probability_text, (image.shape[1] - 315, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Resize the image for video (optional)\n",
    "        image = cv2.resize(image, (854, 480)) # (640, 480)\n",
    "\n",
    "        # Write the annotated image to the video\n",
    "        video_writer.write(image)\n",
    "\n",
    "        # Report progress\n",
    "        if report:\n",
    "            print(f'Images processed: {i + 1}/{n_imgs} ', end='\\r')    \n",
    "\n",
    "    # Release the VideoWriter\n",
    "    video_writer.release()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# base_path = 'data/images'\n",
    "\n",
    "# unique_video_ids = eval_imgs_df['id_video'].unique()\n",
    "# video_imgs = eval_imgs_df[eval_imgs_df['id_video']==unique_video_ids[0]]\n",
    "\n",
    "# image_paths = (f'{base_path}/' + video_imgs['file_path']).tolist()\n",
    "# true_labels = video_imgs['flood'].tolist()\n",
    "# predicted_labels = video_imgs['pred'].tolist()\n",
    "# predicted_probs = video_imgs['prob'].tolist()\n",
    "\n",
    "# output_video_path = 'output_video.mp4'\n",
    "\n",
    "# create_annotated_video(image_paths, true_labels, predicted_labels, predicted_probs, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063fa2e-1076-4354-8071-94cb989fa62f",
   "metadata": {},
   "source": [
    "#### Write video results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d2d17bc-cad5-42ba-aa3e-b24a23fc2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras processed: 21/21 \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_path = 'data/images'\n",
    "base_output_path = 'data/eval-videos'\n",
    "\n",
    "unique_camera_codes = eval_imgs_df['code'].unique()\n",
    "n_cameras = len(unique_camera_codes)\n",
    "\n",
    "for i, code in enumerate(unique_camera_codes):\n",
    "    for true_label in [0, 1]:\n",
    "        camera_imgs = eval_imgs_df[(eval_imgs_df['code'] == code) & (eval_imgs_df['flood'] == true_label)].sort_values('timestamp')\n",
    "\n",
    "        if not len(camera_imgs):\n",
    "            continue\n",
    "\n",
    "        image_paths = (f'{base_path}/' + camera_imgs['file_path']).tolist()\n",
    "        true_labels = camera_imgs['flood'].tolist()\n",
    "        predicted_labels = camera_imgs['pred'].tolist()\n",
    "        predicted_probs = camera_imgs['prob'].tolist()\n",
    "        output_video_path = f'{base_output_path}/{true_label}/{int(code)}.mp4'\n",
    "        \n",
    "        output_video_dir = os.path.dirname(output_video_path)\n",
    "        if not os.path.isdir(output_video_dir):\n",
    "            os.makedirs(output_video_dir)\n",
    "            \n",
    "        create_annotated_video(image_paths, true_labels, predicted_labels, predicted_probs, output_video_path, report=False)\n",
    "        print(f'Cameras processed: {i + 1}/{n_cameras} ', end='\\r')    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf49ef-d2c0-4d4e-b3b3-8d7323b5e300",
   "metadata": {},
   "source": [
    "#### End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910d5ca-b081-4302-9a83-441c09ba1aea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e7b8c-45b2-406e-8dda-8203ca1cd469",
   "metadata": {},
   "source": [
    "### Extra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e4a41-3034-4403-aa08-100547fdaef1",
   "metadata": {},
   "source": [
    "#### Load model with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d239213-dbda-46b5-b2b9-0c9af02ddeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the saved checkpoint file\n",
    "checkpoint_filepath = 'results/best_model_cnn.h5'\n",
    "model1 = models.load_model(checkpoint_filepath)\n",
    "\n",
    "params = {\n",
    "    'data': 'data/images',\n",
    "    'epochs': 25,\n",
    "    'imgsz': 640,\n",
    "    'batch': 32,\n",
    "    'device': [0, 1],\n",
    "    'learning_rate': 0.0001,\n",
    "}\n",
    "\n",
    "img_height, img_width = 640, 640 # 480, 854\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    params['data'] + '/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=params['batch'],\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881bb32-3062-419c-9584-df5e89afeb60",
   "metadata": {},
   "source": [
    "#### Copy images into test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b2d9d4-6cf3-4a7c-8f67-8e55c6e48b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying images to test folders:\n",
      "Processed 1070/1070 files (100.00%) - Found: 1070/1070\r"
     ]
    }
   ],
   "source": [
    "from modules.octa_video_util import copy_images_to_folders\n",
    "\n",
    "base_directory = 'data/images'\n",
    "target_directory = 'data/splits/sgkf-8-1-1-test-videos'\n",
    "\n",
    "dataset = df_images_sample.copy()\n",
    "file_path_field = 'file_path'\n",
    "label_field = 'flood'\n",
    "\n",
    "train_indexes = None\n",
    "test_indexes = list(df_images_sample.index)\n",
    "val_indexes = None\n",
    "\n",
    "copy_images_to_folders(\n",
    "    base_directory, target_directory, dataset,\n",
    "    train_indexes, test_indexes, val_indexes,\n",
    "    file_path_field=file_path_field, tag_field=label_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937acf4b-0c04-4ce1-8312-0dd6e4f737b4",
   "metadata": {},
   "source": [
    "#### Save dataframe of sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44a0f4c4-8480-4292-af09-93b145f3bb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split dataframe saved with shape: (1070, 13)\n"
     ]
    }
   ],
   "source": [
    "target_directory = 'data/splits/sgkf-8-1-1-videos'\n",
    "\n",
    "dataset = df_images_sample.copy()\n",
    "\n",
    "# data_train = dataset.loc[Y_train.index]\n",
    "data_test = dataset.loc[test_indexes]\n",
    "# data_val = dataset.loc[Y_val.index]\n",
    "\n",
    "# data_train['set'] = 'train'\n",
    "data_test['set'] = 'test'\n",
    "# data_val['set'] = 'val'\n",
    "\n",
    "data_split_df = data_test.copy()\n",
    "# data_split_df = pd.concat([data_train, data_test, data_val])\n",
    "\n",
    "data_split_df.to_csv(f'{target_directory}/images.csv')\n",
    "print(f'split dataframe saved with shape: {data_split_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323da71-e11b-4e9a-8228-422ccb08a4fd",
   "metadata": {},
   "source": [
    "#### Count save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d8d220d-7465-410b-9d40-5eab5df4db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 247 823\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "target_directory = 'data/splits/sgkf-8-1-1-videos'\n",
    "\n",
    "# print('train:', len(os.listdir(f'{target_directory}/train/0')), len(os.listdir(f'{target_directory}/train/1')))\n",
    "print('test:', len(os.listdir(f'{target_directory}/test/0')), len(os.listdir(f'{target_directory}/test/1')))\n",
    "# print('val:', len(os.listdir(f'{target_directory}/val/0')), len(os.listdir(f'{target_directory}/val/1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2bb58-41ec-4385-866e-ddf613f65201",
   "metadata": {},
   "source": [
    "#### Set up TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f99b42c9-5609-49cf-a850-0f681e002b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not found. Using CPU.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"GPU is available.\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"GPU not found. Using CPU.\")\n",
    "    \n",
    "# Define mirrored strategy for GPU training\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
