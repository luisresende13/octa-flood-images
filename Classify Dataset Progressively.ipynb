{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b64e293-c7bb-43c6-90e3-bef3c6a1a45a",
   "metadata": {},
   "source": [
    "#### Reload images and videos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4829e75-ae85-4e3e-b477-1dfb348428d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'blob_name', 'blob_size', 'bucket_name', 'file_name', 'code',\n",
      "       'n_folders', 'timestamp', 'folder_structure', 'folder', 'tags', 'url',\n",
      "       'api_url', 'bucket', 'seen'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=10, step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_images = pd.read_csv('data/datasets/images_clean.csv')\n",
    "df_videos = pd.read_csv('data/datasets/videos.csv')\n",
    "\n",
    "# df_images['tags'] = df_images['tags'].apply(json.loads)\n",
    "df_videos['tags'] = df_videos['tags'].apply(json.loads)\n",
    "\n",
    "print(df_videos.columns)\n",
    "print()\n",
    "display(df_videos.index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55493beb-e966-408a-bc13-8aec74787d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_images.index != df_images.sort_values(['id_video', 'frame_index']).index).sum()# .head(45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061157ec-915a-4791-9834-4c2fb19aa9b7",
   "metadata": {},
   "source": [
    "#### Check existence of images in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9902a963-c2c1-45dc-8981-ccec0a3fd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.0 %'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def get_nested_files(folder_path):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path.replace('\\\\', '/'))\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "base_path = 'data/images'\n",
    "\n",
    "images_paths_found = get_nested_files(base_path)\n",
    "\n",
    "images_paths_sample = df_images['file_path'].apply(lambda file_path: f'{base_path}/{file_path}'.replace('\\\\', '/'))\n",
    "files_exist_prct = images_paths_sample.isin(images_paths_found).mean()\n",
    "\n",
    "str(round(files_exist_prct * 100, 2)) + ' %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb1478-69f4-4079-93f5-fe4e6fa4a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/images'\n",
    "\n",
    "image_paths = base_path + '/' + df_images['file_path']\n",
    "\n",
    "files_exist_prct = image_paths.map(lambda x: os.path.exists(x)).mean()\n",
    " \n",
    "str(round(files_exist_prct * 100, 2)) + ' %'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf8e85-147f-4e15-a8ae-1aedf867e8e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0b98e-17fe-4b09-811e-e1eb7372cd32",
   "metadata": {},
   "source": [
    "## Run predictions with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0badd0-aae0-4a72-86ec-b6e23ca7447a",
   "metadata": {},
   "source": [
    "#### Load model with Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5d8675-1d5f-424d-abec-0b109649a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to the folder you want to zip\n",
    "# model_path = f'models/sgkf-8-1-1/weights/best.pt'\n",
    "# model_path = f'models/sgkf-50-25-25-size-2024-rs-2/weights/best.pt'\n",
    "model_path = f'models/full-imbalanced-train/weights/best.pt'\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(model_path)  # load a partially trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2d829-0e11-41fb-b0a2-0ddce906c5f2",
   "metadata": {},
   "source": [
    "#### Run predictions with yolo in batch progressively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93b7db-727c-44e9-b732-a2e840cc9cca",
   "metadata": {},
   "source": [
    "#### Compare time to predict using batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa749b5-3257-4e08-bf2a-9e4033bba260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "path_field = 'full_file_path'\n",
    "\n",
    "df_images[path_field] = f'{base_path}/' + df_images['file_path']\n",
    "\n",
    "n = 15\n",
    "batch = 16\n",
    "batch_paths = df_images[path_field].iloc[:batch].tolist()\n",
    "\n",
    "s = time.time()\n",
    "for i in range(n):\n",
    "    for img_path in batch_paths:\n",
    "        model.predict(img_path, verbose=False)\n",
    "    diff = time.time() - s\n",
    "    avg = round(diff / n, 3)\n",
    "\n",
    "print('Avg-Time Op. 1: ', avg, 's')\n",
    "\n",
    "s = time.time()\n",
    "for i in range(n):\n",
    "    model.predict(batch_paths, batch=1, verbose=False)\n",
    "    diff = time.time() - s\n",
    "    avg = round(diff / n, 3)\n",
    "\n",
    "print('Avg-Time Op. 2: ', avg, 's')\n",
    "\n",
    "s = time.time()\n",
    "for i in range(n):\n",
    "    model.predict(batch_paths, batch=batch, verbose=False)\n",
    "    diff = time.time() - s\n",
    "    avg = round(diff / n, 3)\n",
    "\n",
    "print('Avg-Time Op. 3: ', avg, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d95d85f-5183-4fc4-9697-0f6a10a1b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Interrupted: 1896/247784 | 1.82 min / 516.14 min | time-left: 512.19 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[5.495740581368258e-12, 0.0],\n",
       "  [1.3343072306770676e-11, 0.0],\n",
       "  [2.3521630823641893e-11, 0.0],\n",
       "  [2.2271998481593336e-11, 0.0],\n",
       "  [3.1580519449514455e-12, 0.0],\n",
       "  [8.035421828790634e-13, 0.0],\n",
       "  [1.1554737201768804e-11, 0.0],\n",
       "  [1.863289939008084e-10, 0.0],\n",
       "  [3.139693366405183e-11, 0.0],\n",
       "  [2.5321860927518536e-11, 0.0]],\n",
       " 'Size: 1896')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import clear_output as co\n",
    "\n",
    "def yolo_classify_dataset_batches(model, df, path_field, batch=8, save_each=10, save_path=None):\n",
    "\n",
    "    if save_path is not None:\n",
    "        pred_path = f'{save_path}/pred.csv'\n",
    "        stats_path = f'{save_path}/stats.json'\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    index_start = 0\n",
    "    preds = []\n",
    "    if save_path is not None:\n",
    "        if os.path.exists(pred_path):\n",
    "            preds_df = pd.read_csv(pred_path)\n",
    "            index_start = len(preds_df)\n",
    "            preds = preds_df.values.tolist()\n",
    "\n",
    "    sources = df[path_field].tolist()  # .iloc[index_start:]\n",
    "    n_imgs = len(sources)\n",
    "    s_time = time.time()\n",
    "    \n",
    "    for i in range(index_start, n_imgs, batch):\n",
    "\n",
    "        e_time = time.time() - s_time\n",
    "        e_time_round = round(e_time / 60, 2)\n",
    "        avg_time = e_time / max(1, i - index_start)\n",
    "        expected_finish_time = round((n_imgs - i) * avg_time / 60, 2)\n",
    "        expected_total_time = round(n_imgs * avg_time  / 60, 2)\n",
    "    \n",
    "        co(True)\n",
    "        print(f'image-results-saved: {i}/{n_imgs} | time-running: {e_time_round} min / {expected_total_time} min | time-left: {expected_finish_time} min')    \n",
    "    \n",
    "        sources_batch = sources[i: i + batch]\n",
    "        \n",
    "        try:\n",
    "            pred = model.predict(sources_batch, imgsz=640)\n",
    "            pred = [[pred_i.probs.data[1].item(), pred_i.probs.top1] for pred_i in pred]\n",
    "            preds.extend(pred)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            co(True)\n",
    "            print(f'Process Interrupted: image-results-saved: {i}/{n_imgs} | time-running: {e_time_round} min / {expected_total_time} min | time-left: {expected_finish_time} min')    \n",
    "            return preds\n",
    "            \n",
    "        batch_index = int(i / batch)\n",
    "        if batch_index % save_each == 0:\n",
    "            pd.DataFrame(preds, columns=['prob', 'pred']).to_csv(pred_path, index=False)\n",
    "            print(f'Dataset with results saved | IMAGES: {i + batch} | BATCH: {batch_index} | PATH: {save_path}')\n",
    "    \n",
    "    co(True)\n",
    "    print(f'image-results-saved: {i + batch}/{n_imgs} | time-running: {e_time_round} min / {expected_total_time} min | time-left: {expected_finish_time} min')    \n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "base_path = 'data/images'\n",
    "path_field = 'full_file_path'\n",
    "\n",
    "df_images[path_field] = f'{base_path}/' + df_images['file_path']\n",
    "\n",
    "df = df_images\n",
    "batch = 12\n",
    "save_each = 1\n",
    "save_path = 'data/images-pred'\n",
    "\n",
    "preds = yolo_classify_dataset_batches(model, df, path_field, batch, save_each, save_path)\n",
    "\n",
    "preds[:10], f'Size: {len(preds)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cb429-6d96-477d-bb77-72c5667e5bb0",
   "metadata": {},
   "source": [
    "#### Convert image labels to video labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff6dbf8-9636-40c1-9b6f-afca38f95e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = 'data/images-pred/pred.csv'\n",
    "\n",
    "preds = pd.read_csv(preds_path)\n",
    "\n",
    "df_images_pred = df_images.iloc[:len(preds)]\n",
    "\n",
    "df_images_pred[['flood-prob', 'flood-pred']] = preds.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f88d9-aeb2-46a9-b815-b4e32741d389",
   "metadata": {},
   "source": [
    "#### Post results to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7a21-86ef-4be7-8aba-840f2ba0a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def add_tags(tags, blob_name, bucket)\n",
    "    baseUrl = ''\n",
    "    url_tags = f'{baseUrl}/tags';\n",
    "    data = {\n",
    "        'tags': tags,\n",
    "        'b': bucket,\n",
    "        'blob_name': blob_name,\n",
    "    };\n",
    "    res = requests.post(url_tags, json=data)\n",
    "    if not res.ok:\n",
    "        return None\n",
    "    return res.json()\n",
    "    \n",
    "# tags = ['acúmulo-ia']\n",
    "# bucket = ''\n",
    "# blob_name = ''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
